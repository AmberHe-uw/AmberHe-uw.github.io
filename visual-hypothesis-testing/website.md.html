<meta charset="utf-8">
<!-- Website is 680px wide by default. If we scale iframe by 0.8, we want the width to be 680 / 0.8 = 850-->
<style>
    #wrap1 { width: 680px; height: 680px; padding: 0; overflow: hidden; }
    #wrap2 { width: 680px; height: 600px; padding: 0; overflow: hidden; }
    #frame1 { width: 850px; border: none; }
    #frame1 {
        -ms-zoom: 0.8;
        -moz-transform: scale(0.8);
        -moz-transform-origin: 0 0;
        -o-transform: scale(0.8);
        -o-transform-origin: 0 0;
        -webkit-transform: scale(0.8);
        -webkit-transform-origin: 0 0;
    }
    #frame2 { width: 1545px; border: none; }
    #frame2 {
        -ms-zoom: 0.55;
        -moz-transform: scale(0.55);
        -moz-transform-origin: 0 0;
        -o-transform: scale(0.55);
        -o-transform-origin: 0 0;
        -webkit-transform: scale(0.55);
        -webkit-transform-origin: 0 0;
    }


</style>
# A Visual Guide to Multiple Testing

## Introduction
Research today often involves large amounts of data, providing scientists with many possible hypotheses to explore. Given the wide array of observed patterns in a particular dataset, and the likelihood that many of these patterns are simply noise, we require statistical analyses that can distinguish the signals from the noise. This is the key problem of multiple hypothesis testing.

This blog post introduces the problem of testing multiple hypotheses and reviews some basic error metrics and multiple testing procedures. Our goal is to present this information with visualizations that illustrate the problems and procedures clearly. While our main audience is students and researchers that frequently use statistical tools, this blog should be approachable by anyone with familiarity with undergraduate-level probability and statistics.

We begin by introducing hypothesis testing and explaining the problem with conducting multiple hypothesis tests. We then review global null tests and illustrate different procedures to test this hypothesis. We then introduce different error metrics for effect detection (FWER and FDR) and procedures that gives us error rate guarantees. We conclude by introducing more realistic settings where researchers interact with the data and conduct tests over time and review procedures that gives us error control in these contexts.


## Multiple Testing: Why is it a Problem?

Before we introduce the problem of multiple testing, we briefly review hypothesis testing. Hypothesis testing generally involves providing statistical evidence against some "boring" (null) hypothesis in favor of a more interesting alternative (research) hypothesis. Testing this null hypothesis involves collecting data and then conducting some statistical test to evaluate whether or not the data is consistent with this hypothesis. Because our data is a random sample, it is always possible that we make an error: either that we reject the null hypothesis when its false (Type I Error), or accept it when it's not true (Type II Error). As a result, before conducting our test, we set a level \alpha that specifies the maximum Type I Error Rate that we would be willing to accept were we able to conduct this test an infinite number of times.

Most hypothesis tests generate a test statistic with an associated p-value. A p-value represents the probability under the null hypothesis that we would have observed a test statistic at least as extreme as the one we saw. When the null hypothesis is true, p-values have a uniform distribution. Figure [p-vals] below illustrates this: in the left-hand panel we sample observations from a standard normal distribution (ie mean zero with standard deviation equal to one). The middle panel displays the p-value associated with a hypothesis test of whether the observation is statistically significantly different from zero. In other words, assuming that the true mean is zero (ie that the null hypothesis is true), the p-value represents the probability of seeing our draw from X. Finally, the right hand panel illustrates the empirical cumulative distribution function (CDF) of the p-values. As the number of samples N increases, we can see that the empirical CDF converges to the Uniform CDF, illustrated in the red line. 

![Figure [p-vals]: Overview of p-values.](images/image9.gif)

In many real-world settings, however, we are interested in conducting several hypothesis tests. For example, we might be involved in a clinical trial where we want to test the effect of a new cancer drug on multiple outcomes (eg 6-month, 1-year, and 5-year survival rates). However, once we expand the framework from considering one hypothesis test to multiple hypothesis tests, the statistical procedures for controlling the Type I Error Rates for an individual hypothesis test break down. This setting also requires us to expand our notion of error control to consider multiple tests. A common metric is the family-wise error rate (FWER): this is the probability of rejecting the null when it's true at least once -- or of making at least one false discovery -- among a set of hypotheses.

For example, consider Figure [p-vals] again: even though all the tests are from standard normal (mean zero) draws, when we conduct multiple tests we are increasingly likely to see extreme p-values (i.e., p-values that are very close to 0 or 1). Standard hypothesis testing only controls the probability of making a false discovery for that particular test. When we conduct multiple individual level alpha tests, we are also increasingly likely to make a false discovery -- because p-values are informly distributed. As an analogy, imagine you have bag of 100 M&Ms where only one is red and 99 are brown. You have a 1/100 chance of getting the red M&M on the first draw. If you do not, imagine that you put the brown M&M back in the bag, and try again. While you have a 1/100 chance of getting the red M&M for each draw, intuitively across all draws it is increasingly likely you will choose the red M&M at least once. In fact, you are  guarateed to choose it if you do this procedure an infinite number of times. The same is true with multiple hypothesis testing: as we increase the number of hypothesis tests, we are guaranteed to make a false discovery.

![Figure [multiple-tests]: Testing multiple hypotheses increases the number of false discoveries.](images/image7.png width=90%)

Figure [multiple-tests] above illustrates this. The x-axis represents the number level 0.05 hypothesis tests we conduct. The y-axis depicts the FWER, and the points on the plot represent the empirical FWER calculated across 1000 simulations. The red-dashed line represents the 0.05 line; it is clear that the probability of making at least one false discovery is well controlled at 0.05 given one hypothesis test, as we would expect. However, as we increase the number of tests, the FWER increases rapidly and is nearly 1 when we reach even fifty tests. In other words, we are almost guaranteed to make at least one false discovery as when conducting as few as fifty tests. As a result, we must treat settings where we conduct multiple hypothesis tests differently from the single hypothesis testing framework.

## Global Null Testing

Given a set of p-values, a global null seeks to detect whether any of the hypotheses can be rejected. In particular, the global null tests the null hypothesis that all of the tests are null against the alternative that at least one of the tests is not null. This type of test is useful if we’re not interested in determining which specific hypotheses we should reject, but rather whether we may have found any effects among a set of hypotheses. For example, we might be applying for grant funding and have already conducted a pilot study that involved some intervention and multiple outcomes, but our small sample size was small. As a result, we might worry about our power to detect specific effects; nevertheless, we may be able to show that there was some effect to motivate further research funding for this intervention. More formally, if we have $n$ hypothesis we can write the null and alternate hypotheses as follows:

$$
H_{1,0} = H_{2, 0} = ..., = H_{n, 0} = 0
$$

$$
H_a: \exists k \in \{1,2,...,n\} \texttt{ such that } H_{k} \ne 0
$$

Like the individual hypothesis test setting, our goal here is to control the probability of falsely rejecting the null when it's true at level $\alpha$. One simple idea for a global null test is to take a set of p-values, calculate the minimum p-value, and threshold it at some level related to $\alpha$. If the minimum p-value falls below this threshold then we can reject the global null hypothesis. This is the logic behind the the Bonferroni and Sidak tests. The Bonferroni threshold is $\alpha/n$, while the Sidak correction is $(1 - (1-\alpha)^{1/n})$. This is essentially the same procedure as in the single hypothesis test setting; however, we decrease the rejection region to account for the fact that we're considering multiple hypotheses.

![Figure [global-null]: Global Null Tests.](images/image1.gif)

Figure [global-null] illustrates that these thresholds rapidly decrease as the number of hypotheses increase from 1 to 10; both Bonferroni and Sidak give similar thresholds, though Sidak is slightly higher. The key difference is the assumptions under which the test is guaranteed Type I Error control. In particular Bonferroni makes no assumptions about the dependence of the p-values. For example, if your p-values all come from tests using the same dataset, frequently rejecting one hypothesis test can be predictive of rejecting another hypothesis test (again assuming that the null hypothesis is true). In this case your p-values are positively dependent. By contrast, the Sidak correction assumes that the p-values are independent. This might be the case if you're conducting a meta-analysis of studies that all used different datasets. Nevertheless, in practice both Bonferroni and Sidak give very similar rejection thresholds.

The Bonferroni and Sidak tests work particularly well when we expect a few strong signals among a set of p-values. However, we might also want tests that perform well in scenarios with many weak signals. While Bonferroni and Sidak only look at the minimum p-value, we could also imagine coming up with global null tests that are functions of all of the p-values. Some other tests along these lines are Fisher’s combination test, the KS-test, and Higher Criticism. The details of these tests are beyond the scope of this blog post; however, we encourage the reader to seek more information about these tests if they’re interested.

## Closed Testing and FWER Control

We are often also interested in procedures that take a set of p-values from indstividual hypothesis tests and return some subset that we are confident are true effects. In other words, we want to know which tests among a set of tests were true non-nulls. We first consider procedures that give guarantees on the FWER. These procedures control the probability that there are no false discoveries in the rejected set of hypotheses at $1 - \alpha$. It turns out that we can use any level $\alpha$ global null test to output a procedure that has level alpha FWER control under a framework called Closed Testing (<cite><a href="https://academic.oup.com/biomet/article-pdf/63/3/655/756258/63-3-655.pdf?casa_token=ENRbwoxJnuEAAAAA:pHw6elCe3nr_w4J3Xz6kqUk974_r5SRXG1ipYiUGqF39jRcoVzmraUcULl16idDUAgqLKlG0TDpH"><i>Markus et al. 1976</i></a></cite>).

We begin by creating a directed acyclic graph (DAG) that contains the power set of the p-values (the set of all subsets of the original set). The graph then begins with the node that contains all n p-values. This node has n children: each of the subsets of the original n p-values containing n-1 p-values. Similarly, these nodes also have children: all subsets containing n-2 nodes, and so on. Arrows connect a parent node to a child node only if the child node contains at least one of the hypotheses in the parent node. The final layer of the graph (the leaves) contains nodes with each individual hypothesis: our goal is to determine which of these leaves to reject while controlling the FWER.

The closed testing procedure works as follows:
* For each node (ie set of hypotheses):
* Apply any level alpha global null test and reject if the global null p-value associated with the global null test is less than alpha. This creates an initial set of rejected nodes and nodes that we failed to reject.
* For each initially rejected node:
    * If the node has a parent that we failed to reject, don’t reject this node


We can reject the final set of leaves with FWER control at level $\alpha$. This is true for any global null hypothesis test that we might input, and we can choose any global null test for each node. However, each global null test makes assumptions about the dependency structure of the p-values, and these assumptions carry forward to each node we test.

Figure [closed-testing] below illustrates the Closed Testing paradigm with an example where we’ve conducted five hypothesis tests with five associated p-values. The animation then shows the closure procedure: if any of the nodes has a parent that we failed to reject, we also fail to reject this nodel. We initially reject the orange nodes and fail to reject the blue nodes. The animation shows us checking each parent node and ``un-rejecting'' any children that we rejected if we failed to reject the parent. In this case we fail to reject any of the leaves after closure.

![Figure [closed-testing]: Graph Closure Procedure.](images/image5.gif width=80%)

The key drawback of using closed testing in practice is that it requires conducting up to $2^n$ global null tests. Several specific procedures exist that give guarantees on the FWER that are computationally much faster.

We've actually already covered two such procedures that we can use to control the FWER: Bonferroni and Sidak’s method. While previously we only considered using them for a global null test, it turns out that we can apply the thresholds procedure to all of the p-values and get FWER. For example, for Bonferroni we can reject any hypothesis that has a p-value of less than $\alpha/n$, giving us a set of hypotheses then has FWER control at level alpha. If we think that the p-values are independent, we can use the Sidak threshold for any of the p-values. 

We can actually strictly improve on Bonferroni's method to get a more powerful test -- even assuming arbitrary dependence of the p-values. This is called Holm's method. We outline this method below when considering $n$ p-values, which we first order from smallest to largest. Then

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
for j = 1, ... , n
    if p[j] < alpha/(n-j+1)
      reject hypothesis j
    else
      stop
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Figure [holms] compares Bonferroni and Holm's method and compares the rejection sets given n p-values. We consider the case where all of the p-values are non-nulls. In the left-hand plot, we see that the Bonferroni procedure rejects the first seven p-values, but fails to reject the remaining non-nulls. Notice that the Bonferroni cutoff (the gray horizontal line) is constant across the orderings; by contrast, Holm's procedure increases the cutoff (the red line) as we consider subsequent p-values (again assuming we haven't rejected any of the previous p-values). The left-hand plot shows a case where Holm's procedure rejects all twenty tests, while Bonferroni only rejects the first seven. The plot on the right-hand shows a case where Bonferroni and Holm's procedure return the same rejection set. Notice on the right-hand plot that once we fail to reject a hypothesis under Holm's procedure, we fail to reject all remaining hypotheses, even though subsequent p-values that fall below alpha/(n-j+1) (the red line). Because Holm's procedure allows us to increase our rejection threshold for each subsequent test in a series of hypothesis tests, the procedure is more powerful than Bonferroni, and should therefore always be preferred in practice. 

![Figure [holms]: Bonferroni vs Holm's Procedure.](images/image11.png width=90%)

## Graphical Procedures

The previous procedures give us guarantees on the FWER; however, these tests are typically quite conservative. That is, the procedures often return very few discoveries. We can guarantee FWER control and increase our power to detect effects if we're willing to incorporate our prior knowledge or structure the problem in some intelligent way. While many methods exist to do this, here we cover the basics of graphical methods here. These methods allow us to control the FWER by passing around any "excess error budget" to other tests, potentially improving our power to detect effects -- if our assumptions are sensible.

We begin with the simplest case, the fallback procedure. This procedure is relevant if we are able to a priori order our hypotheses in some meaningful way. For example, say we have n hypotheses that we wish to test in some order because we have ideas about which hypotheses are most likely to be non-nulls. We can then test these first with lower error budgets, and then pass the previous error budget to subsequent hypothesis tests. This allows us to reject subsequent tests with higher p-values (and improving our power to detect effects).

![Figure [fallback]: Fallback Procedure.](images/image10.gif)

The animation above in Figure [fallback] depicts this type of procedure. Each node in the graph represents a hypothesis test with an associated p-value, indicated by the size of the blue circles. The nodes are then connected by directed edges; these directed edges serve to order the hypotheses. For example, in the graph above we first test $H_0$, then $H_1$, and so on until $H_{N-1}$.

We assume that we have some total error budget ($\alpha$). This error budget again represents the FWER level we would like to control. Before testing anything, we divide this error budget up among the hypotheses. For example, we might use the Bonferroni threshold $\alpha/n$. However, we can divide $\alpha$ in any way as long as $\sum_i \alpha_i = \alpha$. The orange circles represent this initial allocation. If the p-value (blue circle) is smaller than the budget (orange circle), then we reject the hypothesis. The key idea of fallback procedures is that, if we reject hypothesis $H_1$, we get to "save" the error budget from the previous test $\alpha_1$ and transfer it to the next hypothesis $H_{2}$. That is, we test $H_{2}$ at level $\alpha_1 + \alpha_{2}$. 

One special case of fallback procedures is where the initial budget for all hypotheses, except H_0, is zero. This case is illustrated in the first figure. Note that, if the error budget is zero, we will never reject the hypothesis, regardless of the p-value. Thus, we don't need to conduct a hypothesis test unless it has an error budget greater than zero. In the figure below, this means that we never need to conduct tests $H_3$, $H_4$, and $H_5$.

![Figure [fallback]: Fallback Procedure on different p-values.](images/image6.gif)

We can generalize these procedures by pass along excess error budget across any DAG. That is, we can connect multiple hypotheses with directed edges, allowing excess error budget from one test to be passed to multiple subsequent tests. In fact, it turns out that both Bonferroni and Holm's procedure have graphical representations! The animation below in Figure [graphical] depicts a more general scenario:

![Figure [graphical]: Graphical Procedure.](images/image3.gif width=50%)

The key to making graphical procedures work well is having good underlying assumptions about the orderings of the hypothesis and smart initial allocations of the error budget. While you're guaranteed to get FWER control regardless, you can worsen the power of your test if you do a bad job at this, just as you can improve your power if you do a good job.

More generally other methods exist that improve the power of Bonferroni if you're willing to incorporate prior knowledge. For example, we can also run weighted versions of Bonferroni, where we allocate less error budget to hypotheses that we think are more likely to be non-nulls. This then also us to reject other hypotheses at larger p-values. These types of weights are known as prior weights. Similarly, we can use weights to differentially penalize false positives from certain hypotheses; for example, some hypotheses are much more expensive to follow-up on than others so we wish to have stronger error control for these hypotheses. We can then weight these hypotheses using penalty weights.

The key takeaway is that we can get FWER control and improve our power to detect hypotheses relative to simply running Bonferroni. Structuring our problems using graphical procedures is one way to do this. We can also consider prior and penalty weighted versions of Bonferroni.

## False Discovery Rate

The FWER is a very strong notion of error control: in particular, procedures that control the FWER don’t take into consideration how many hypotheses we are testing. Consequently, even if we test 1,000 hypotheses, procedures that control FWER still control the probability that we make at least one false discovery.

In settings with many hypotheses, we may wish to have a less stringent version of error control. One popular metric to target is the False Discovery Proportion. The FDP is the ratio of the number of false rejections (Type I Errors) to the total number of rejections. Contrast this to the False Discovery Rate (FDR) is the expected value of the FDP for a given procedure. The most popular procedure targeting the FDP, the Benjamini-Hochberg (BH) procedure, actually controls the FDR (<cite><a href="https://www.jstor.org/stable/pdf/2346101.pdf?casa_token=D3klz0vfVoMAAAAA:_uaSHD00FRBcMpoVZfTVleu4OiktJ42GRz9UZcxzlyXG3pU_c6b1zZBVZQQyZhg-I3rIg8WFhoY1YuM3nhZBn2qzjxlyKKXzf9OXGtpeK6xh7rj02PY"><i>Benjamini & Hochberg 1995</i></a></cite>).

We briefly outline the BH procedure. Given a set of M hypotheses:
* Sort the M p-values and order $1, \cdots, P$
* Calculate the maximum index k ($\hat{k}$) such that $p_k \le alpha*k/M$
* Reject all p-values below $\hat{k}$

![Figure [FDR-1]: Controlling the False Discovery Rate with Bonferroni, Holm and Benjamini-Hochberg Procedures](images/image2.png width=90%)

Figure [FDR-1] above shows the relationship between the BH procedure and Bonferroni and Holm. We ran 1,000 simulations where we ran Bonferroni, Holm, and BH on p-values where 200 were non-nulls and 800 were nulls, where the non-null test statistics were drawn from N(3, 1) and the nulls were drawn from N(0, 1). For each of the 1,000 simulations we calculated the $\hat{k}$ (largest rejected index of the ordered p-valued) at level alpha = 0.1 and the FDP associated with each index (that is the total number of non-nulls divided by the index number). We then averaged these values across the 1,000 simulations to produce the figure above.

The graph illustrates that these procedures all involve rejecting all p-values below some $\hat{k}$ -- specified by the blue (Bonferroni), green (Holm), and red lines (BH). Bonferroni and Holm allow far fewer rejections, as they are seeking to control the FWER. By contrast, BH almost triples the number of discoveries (warning: in this simulation, not in general), at the cost of increasing the mean FDP ($\approx$ FDR) to near 0.1.

An important caveat about the BH procedure is that it controls the FDR -- the expected FDP -- at level alpha. However, controlling the FDP in expectation is very different from controlling the FDP in probability. Figure [FDR-2] below depicts simulations meant to illustrate this distinction. For these simulations we again sample 200 observations from a random normal distribution with mean A and variance 1 and 800 observations from a random normal with mean 0 and variance 1 (to be clear, the observations from N(A, 0) are true non-nulls and the N(0, 1) are true nulls). We then run the BH procedure to generate a rejection set and calculated the FDP of this set. We ran this 10,000 times each for A = 1, 2, 3 and for alpha = 0.05 and 0.1 (that is, controlling the expected FDP control). Finally, we plot the distribution of the FDPs below. The empirical mean FDP is indicated by the red vertical line, while the blue vertical line indicates the FDR control of the procedure.

![Figure [FDR-2]: Simulating the False Discovery Proportion](images/image4.png width=90%)

Notice that the distribution of FDP can actually be fairly widely distributed around the means, and in fact is often larger than the expectation we’re seeking to control (indicated by the blue line). The key message is that FDR control may often be more desirable than the FWER; however, the BH procedure, the most popular procedure for this setting, does not actually have any probabilistic guarantee on returning a set with FDP control at level alpha. As a result, BH may return a set with much higher or lower FDP than your targeted value of $\alpha$.

## Intro to online and interactive testing

We've now introduced the problem with multiple testing, covered different types of tests and error metrics we can consider in this setting, and reviewed some procedures to get control of these errors. However, the previous sections all assume that you’ve completed all of your hypothesis tests and that you've specified the level $\alpha$ error control you desire before running these procedures.

Unfortunately the practice of statistics often differs from the theory. For example, FDR control sounds desirable, but what if we choose $\alpha$, run the BH procedure, and find that we have very few discoveries? A natural response would be to increase $\alpha$ to get a larger set. Unfortunatey, we cannot do this because now our $\alpha$ depends on our data! As a result the subsequent BH results are no longer valid: once we allow our $\alpha$ to depend on the data, the FDR associated with the output subsequent BH procedure is no longer necessarily controlled at level $\alpha$. If this isn't intuitive, consider the analogy to the single hypothesis testing case: we can't choose an $\alpha$, run a hypothesis test, find that the results are insignificant, and then change $\alpha$ so that we reject the null hypothesis: we are only guaranteed Type I Error control for an $\alpha$ that we choose before running the hypothesis test. Another way to think about this is that $\alpha$ is a non-random quantity; however, once we allow $\alpha$ to depend on our data it becomes another random variable. Yet a data-dependent choice of $\alpha$ is clearly desirable, particularly when we would like some number of output hypotheses. That is, we often would like to have an "interactive" testing procedure that has statistical gaurantees.

Another common scenario in practice occurs when we have not completed all of our hypothesis tests at once; instead, we conduct our tests over time. In many cases we may not even know how many tests we will eventually run. This setting is often referred to as "online" testing. Fortunately, we have ways to address both the interactive and online testing problems. We briefly introduce some of the results from this literature. However, we caveat that this is a very active area of research that is largely beyond the scope of this introduction.

### Interactive FDR Control

We first consider the case where we desire FDR control but don't want to choose $\alpha$ beforehand; instead, we would like to explore the data and make some data-dependent choice of $\alpha$. In other words, we seek "uniform" guarantees on the FDP for any subset of hypotheses. Assume we are testing n hypotheses; as we noted in the Closed Testing section above, there then exist $2^n$ possible subsets of these hypotheses. We can get a probabilistic guarantee on the FDP for each of these subsets! For any subset $S$, we have that the probability that the $FDP(S) \le \hat{FDP}(S)$ is greater than $1 - \delta$, where $\hat{FDP}$ is a data-dependent estimate of the FDP (<cite><a href="https://projecteuclid.org/download/pdfview_1/euclid.ss/1330437937"><i>Goeman and Solari 2011</i></a></cite>).

The procedure is a re-interpretation of Closed Testing. Recall that Closed Testing creates a DAG of all possible subsets of hypotheses. An estimate the FDP for each subset is the cardinality of the largest subset of S that isn’t rejected by Closed Testing over the number of hypotheses in S. That is, choose any node with some interesting subset of hypotheses. Then look at of the children of this node and locate the node with the largest cardinality -- that is, the largest number of hypotheses -- that is not rejected. Our estimate of the FDP ($\hat{FDP}$) is simply this number over the number of hypotheses in the node of interest. This estimate $\hat{FDP}$ is then greater than the true FDP with high probability. Researchers can therefore use the Closed Testing output to choose any data-dependent subset S with associated $\hat{FDP}$. The challenge is that with $2^n$ total subsets, parsing through these and choosing which subset (or subsets) to report could be a daunting task. Interactive data visualization tools would be a useful advance here to help researchers implement this procedure in practice.

### Online Testing

Many real-world statistical applications involve testing hypotheses over time. For example, clinical trials involve several randomized experiments that occur in different stages. Online advertisers run many “AB tests” of website design and marketing throughout the year to improve web-traffic and sales. In these settings we likely don’t know beforehand how many tests or trials we’re going to have to run. Is it still possible to get guarantees on the FWER and FDR? The answer is yes! Several procedures exist for both, and new ones are currently being developed. Because FWER control is such a conservative notion of error control -- we're essentially trying to prevent even one false discovery -- it is often less interesting to practitioners, so we omit to cover online versions of Bonferroni and Holm (which control the FWER). Instead we review the LORD procedure (<cite><a href="http://papers.nips.cc/paper/7177-a-framework-for-multi-armedbandit-testing-with-online-fdr-control.pdf"><i>Yang et al 2017</i></a></cite>), which gives us guarantees on the FDR at level $\alpha$. We caveat that these procedures assume that the hypotheses (and their associated p-values) are independent under the null. The procedure works as follows:

* Pick a sequence $\gamma_i$ such that $\sum_{i=1}^{\infty} \gamma_i = 1$.
* Set a sequence $\alpha_i$ such that $\sum_i \alpha_i \le \alpha$.
* For $i = 1, \cdots$
  * Test $H_i$ at level $\alpha_i$
  * If we reject $H_i$
    * Increase the budget for $H_{i+1}$ to $\alpha_{i+1} = \alpha \gamma_{i+1} + \alpha \gamma_{i}$.
  * Else
    * Keep the budget for $H_{i+1}$ at level $\alpha_{i+1} = \alpha \gamma_{i+1}$

Figure [LORD] below depicts this procedure. The gray bars represent p-values and the orange line represents the threshold we use for each p-value. The green line represents the sequence of thresholds we would have if we don’t reject any further tests. However, as the illustration shows, once we reject a test, we’re able to increase our threshold by $\alpha$.

![Figure [LORD]: Online Testing](images/image8.gif)

## Conclusion

Multiple testing problems occur frequently in research and industry. For example, the field of genomics often wishes to test which genes are associated with some disease. Economists frequently conduct RCTs to determine whether some intervention has an effect on different employment outomes. The pharmaceutical industry conducts clinical trials over time to determine whether newly developed drugs can cure or ameliorate diseases. Marketers conduct numerous A/B tests both simultaneously and over time to try to increase sales for their companies.
        
More broadly all statistical research contributes to a massive a multiple testing problem: every day different teams of researchers test hypotheses; while individually each test may have Type I Error Control at level $\alpha$, taken together as a body of work, we know that even if there are no true effects, we will conclude far more than $\alpha$ percent of these hypothesis tests represent False Discoveries. This statistical fact alone may account for much of the so-called "Reproducibility Crisis." [cite the Stat Seminar speaker] Yet correcting every hypothesis test we conduct to account for all other hypothesis tests is an unreasonable ask -- and even determining how to group hypotheses and then correct for them is not necessarily obvious. Nevertheless, researchers should be aware of the dangers of multiple testing. We hope that this blog post will help introduce these procedures into common study designs to at least mitigate the statistical problems with conducting multiple hypothesis tests. 

## Appendix A: Standard Testing FDP Application

The interactive plot below allows the reader to see how FDP control changes under different procedures described above, including Bonferroni, Sidak, Holm, BH, and no corrections (naive), under different simulation settings. 

<div id="wrap1">
<iframe id="frame1" src="https://kouyangcmu2020.shinyapps.io/36743_app/" scrolling="no" height=850px></iframe>
</div>

Notice that the FDP of the naive hypothesis rejection methodology steadily grows above the desired level of FDP control, while that of the methodologies that account for multiple comparisons remains under control. We also observe that under all simulation parameters, Bonferroni, Sidak, and Holm procedures, which control FWER, are all extremely conservative. On the other hand, BH is much less conservative and more powerful.

## Appendix B: Online Testing FDP Plot

<div id="wrap2">
<iframe id="frame2" src="https://kouyangcmu2020.shinyapps.io/onlie_fdr_app/" scrolling="no" height=1200px></iframe>
</div>

In the above post we discovered one way to control the FDR in an online setting: the LORD procedure. However, several other procedures exist with the same goal. The application above allows user to investigate the performance of the LORD procedure against other procedures in different simulation settings. 

We briefly outline these algorithms. The alpha-investing algorithm here is a variant of the alpha-investing algorithm from <cite><a href="https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2007.00643.x?casa_token=wKgNTDauC8kAAAAA%3AYIPme4GlldxttE_nWzJd9r2v2URYGAoS6v3R3H6ECWFAxkBaEc2kWIn9PxdL_Cx4Z6d8mgGgjwMkZXOx"><i>Foster and Stine (2008) </i></a></cite>, which guarantees FDR control under independent p-values. LOND stands for (significance) Levels based On Number of Discoveries. LORD++ is a more powerful version of LOND, which represents a version of the algorithm dubbed (significance) Levels based On Recent Discovery that has been improved by <cite><a href="http://papers.nips.cc/paper/7177-a-framework-for-multi-armedbandit-testing-with-online-fdr-control.pdf"><i>Ramdas et al (2017)</i></a></cite>. <a href="https://arxiv.org/abs/1802.09098"><i>Ramdas et al (2018) </i></a></cite> also proposed the SAFFRON procedure, which provides an adaptive method of online FDR control that includes a variant of alpha-investing. By increasing the number of sequences and experiments in the simulation, the SAFFRON procedure uses up its error budget the most compared to the other procedures being examined, while the other methods all seem to be relatively similar in their Type-1 error control, though the alpha-invest algorithm seems to have more large outliers, suggesting it is a higher variance procedure.


<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
